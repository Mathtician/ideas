\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage{bookmark}
\usepackage{physics}
\usepackage{hhline}
\graphicspath{{images/}}
\hypersetup{
  colorlinks=true,
  urlcolor=blue,
}
\begin{document}

\title{Notes on $\mathcal{G}$}
\author{Aresh Pourkavoos}
\date{}
\maketitle

\tableofcontents
\newpage

\section{Introduction}
Geometric algebras (or real Clifford algebras),
denoted by $\mathcal{G}$, provide a unified framework
for diverse topics in math and physics,
mostly centered on transformations in $n$-dimensional space.
These include but are not limited to
the familiar reflections, rotations, and translations of Euclidean space;
the hyperbolic rotations or Lorentz boosts which appear in special relativity;
the circle inversions used in compass-and-straightedge constructions;
and the perspective transformations which find applications in computer vision.
They contain objects which represent area, volume, and beyond
in addition to the vectors which represent length.
When Clifford algebras are constructed using complex numbers instead of reals,
they capture the unitary transformations and spinors of quantum mechanics.
When combined with calculus, geometric algebra encompasses the theory of differential forms.

\subsection{Linear algebra review}

Since geometric algebra is an extension of linear algebra,
a review of the basic ideas of the latter
will be helpful in understanding the former.
For this section, scalars are in italics and vectors are in bold.
However, geometric algebra adds many other objects to scalars and vectors
(and in fact, scalars are themselves represented as vectors),
so the distinction between them becomes less meaningful.
For this reason, all variables will be italicized in later sections.

A vector space consists of a set of scalars $F$ and a set of vectors $V$.
$F$ must be a field, meaning that
scalars may be added, subtracted, multiplied, or divided
(except by zero) to produce another scalar.
The functions of addition and multiplication,
as well as the additive and multiplicative inverses
which make subtraction and division possible,
must obey certain properties for $F$ to be a field,
e.g. $a+b=b+a$ for all scalars $a$ and $b$.
These notes will focus mostly on the cases
where $F=\mathbb{R}$, the field of real numbers.
For $V$ to form a vector space,
vectors must be able to be added to one another
or multiplied by a scalar to produce another vector.
These functions of vector addition and scalar multiplication, too,
must obey certain properties,
e.g. $a(\vb{u}+\vb{v}) = a\vb{u}+a\vb{v}$
for all scalars $a$ and vectors $\vb{u}$ and $\vb{v}$.
(This property can be stated as
``the scalar product respects vector addition''.)

Every vector space has a basis, a set of vectors
which may be multiplied by scalars and added to each other
to uniquely produce any vector in $V$.
The size of the basis is called the dimension of $V$.
These notes are concerned with vector spaces with finite dimension $n$,
so for a given basis $\{\vb{e}_1, \ldots, \vb{e}_n\}$,
every vector $\vb{v} \in V$ can be uniquely represented as  $v_1\vb{e}_1+\hdots+v_n\vb{e}_n$,
where $v_1, \ldots, v_n \in F$.
The vector space is thus called $F^n$,
or specifically $\mathbb{R}^n$ in the case of geometric algebras.
For short, $\vb{v}$ is often written as a column listing these scalars:
\[\vb{v} = \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}\]

Vectors are basic objects in linear algebra, along with matrices,
which represent ways to transform vectors
in a way that respects vector addition and scalar multiplication.


\section{Definition}

\subsection{Algebras}

Despite everything than can be shown about vector spaces alone,
they do not define a way to multiply two vectors.
This lack gives rise to the idea of an algebra:
a vector space equipped with an additional operation
which multiplies two vectors to produce another vector.
This vector product is constrained by only one rule:
it must be linear with respect to both arguments,
i.e. when one argument is fixed,
it must respect vector addition and scalar multiplication in the other argument.
In general, the vector product does not need to be commutative, associative, etc.
However, thanks to the constraint of linearity,
it is only necessary to define the products of the basis vectors with each other,
and all other values of the product become defined.
For example, consider a two-dimensional vector space
with a basis $\{e_1, e_2\}$
and two vectors within that space
$u = \begin{bmatrix} a \\ b \end{bmatrix}$
and $v = \begin{bmatrix} c \\ d \end{bmatrix}$.
\footnote{As mentioned in the review of linear algebra,
  vectors will be italicized like other variables from this point forward.}
If $e_1e_1$, $e_1e_2$, $e_2e_1$, and $e_2e_2$ are known,
then
\[
uv
= \begin{bmatrix} a \\ b \end{bmatrix}
\begin{bmatrix} c \\ d \end{bmatrix}
= (ae_1+be_2)(ce_1+de_2)
= ace_1e_1+ade_1e_2+bce_2e_1+bde_2e_2.
\]
Scalars must commute with the vector product,
hence $ace_1e_1$ is written above instead of $ae_1ce_1$.
However, $e_1e_2$ and $e_2e_1$ are distinguished
and not combined as like terms,
since the vector product itself is not necessarily commutative.

\subsubsection{The cross product}

To give an example of an algebra (and a non-commutative one at that),
consider the cross product $\times$, which forms an algebra over $\mathbb{R}^3$.
The cross product returns the following for all pairs of basis vectors,
where $0$ is the null vector:

\begin{center}
  \begin{tabular}{|c||c|c|c|}
    \hline
    $\times$ & $e_1$ & $e_2$ & $e_3$ \\ \hhline{|=#=|=|=|}
    $e_1$ & $0$ & $e_3$ & $-e_2$ \\ \hline
    $e_2$ & $-e_3$ & $0$ & $e_1$ \\ \hline
    $e_3$ & $e_2$ & $-e_1$ & $0$ \\ \hline
  \end{tabular}
\end{center}

As shown in the table,
the cross product is anticommutative rather than commutative,
i.e. $u \times v = -v \times u$ for all basis vectors
and, by extension, for all vectors $u$ and $v$.
This vector product is widely used in physics
to calculate quantities related to rotation, such as angular momentum.
However, it cannot be generalized to higher dimensions,
but it can be restricted to work in two
by omitting the last row and column of the table.
Notice, though, that the result will simply be a scalar multiple of $e_3$,
so the cross product in 2D effectively returns a scalar instead of a vector.
The cross product, then, has the peculiar property
that it returns a scalar in two dimensions,
a vector in three dimensions,
and nothing in any other dimension.
Additionally, the components of vectors usually contain a unit of length,
e.g. meters or meters/second,
which makes sense geometrically, as the components of vectors are segments of a certain length.
The output of the cross product of two such vectors, on the other hand, has units of area,
so scaling the input vectors up or down will affect the length of the cross product---
not just in the absolute sense, but also relative to the lengths of the original vectors!
In this way, the cross product depends on the length of a unit basis vector
and is not ``intrinsic'' to the two vectors being multiplied.

Geometric algebra defines an alternative to the cross product
which works in any number of dimensions,
which always returns the same type of object,
and which has a more natural geometric interpretation.
This is known as the exterior product and will be discussed in later sections.

\subsection{Quadratic forms}

\subsubsection{Signature}

\section{Properties}

\subsection{Conic sections: a tangent}

\section{Representing transformations}

\subsection{Reflections, rotations, and translations}

\subsection{Symmetries of spacetime}

\subsubsection{Classical mechanics}
Classical (Newtonian) mechanics relies on three principles
governing the symmetries of space and time.
For this section, ``frames of reference'' refer specifically to inertial frames of reference,
namely those of observers which are not accelerating.
\begin{enumerate}
\item Shifts between frames of reference are performed by linear transformations.
\item The laws of physics remain the same for all frames of reference.
\item The passage of time is absolute for all frames of reference.
\end{enumerate}
\subsubsection{Special relativity}
% Don't write anything here until the other section is done!

\subsection{Conformal transformations}

\subsection{Conics, revisited}

\section{From \texorpdfstring{$\mathbb{R}$}{R}
  to \texorpdfstring{$\mathbb{C}$}{C}
  to \texorpdfstring{$\mathbb{H}$}{H}}

So far, the components of multivectors have been real numbers,
and the Clifford algebra has been generated by a quadratic form.
With a few tricks related to representations, we can extend what we know
about Clifford algebras over the real numbers $\mathbb{R}$
to related algebras over the complex numbers $\mathbb{C}$
and algebra-like objects over the quaternions $\mathbb{H}$.

\subsection{\texorpdfstring{$\mathbb{R}$}{R} to \texorpdfstring{$\mathbb{C}$}{C}}

One might think that real Clifford algebras can be extended to complex ones
just by extending the domain of scalars to complex numbers.
Then, the square of a vector may be generally written $v^TSv$ as before,
where $S$ is a symmetric matrix,
the only difference being that $v$ and $S$ may now have complex elements.

However, not all complex symmetric matrices are diagonalizable.
For example,$
\begin{bmatrix}
  2i & 1 \\
  1 & 0
\end{bmatrix}$
has only one eigenvalue (namely $i$),
whose eigenspace is only one-dimensional.

https://www.netlib.org/utk/people/JackDongarra/etemplates/node263.html

Since Clifford algebras are, at their core, defined by how they square vectors,
each of these paths begins by choosing a method to do so for complex-valued vectors
in a way that generalizes quadratic forms over the reals.
The first of these is:

\subsubsection{Quadratic forms over \texorpdfstring{$\mathbb{C}$}{C}}

However, even if $Q \Lambda Q^{-1}$ is not possible,
what about $Q \Lambda Q^T$?

\subsubsection{Hermitian forms}

A complex number $a+bi$ can be represented by the $2 \times 2$ real matrix
$\begin{bmatrix} a & -b \\ b & a \end{bmatrix}$
so that complex addition, multiplication, and inverses
map to the corresponding operations on matrices.
For example,
\begin{align*}
  (a+bi) (c+di) &= (ac-bd)+(bc+ad)i, \\
  \text{and} \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
  \begin{bmatrix} c & -d \\ d & c \end{bmatrix} &=
  \begin{bmatrix} ac-bd & -(bc+ad) \\ bc+ad & ac-bd \end{bmatrix}.
\end{align*}
This matrix can also be thought of as a linear transformation of the plane,
namely a scaling and rotation.

Likewise, an $m \times n$ complex matrix can be represented as a $2m \times 2n$ real matrix,
with each entry of the complex matrix becoming a $2 \times 2$ block of the real matrix.
For example, the $2 \times 2$ complex matrix
\begin{align*}
  \begin{bmatrix} 1+2i & 5+6i \\ 3+4i & 7+8i \end{bmatrix}
\end{align*}
may be represented as the $4 \times 4$ real matrix
\begin{align*}
  \begin{bmatrix}
    1 & -2 & 5 & -6 \\
    2 & 1 & 6 & 5 \\
    3 & -4 & 7 & -8 \\
    4 & 3 & 8 & 7 \\
  \end{bmatrix}.
\end{align*}
Since breaking down matrices into blocks, multiplying them as units,
then adding the results together is a valid method of matrix multiplication,
the complex matrix product is preserved with this real representation.

\begin{center}
  \begin{tabular}{|c|c|}
    \hline
    $\mathbb{R}$ & $\mathbb{C}$ \\ \hline
    $q(x) = x^TSx$ & $h(z) = z^HAz$ \\ \hline
    $S = S^T$ & $A = A^H$ \\ \hline
    $S = Q \Lambda Q^T$ & $A = U \Lambda U^H$ \\ \hline
    $Q^TQ = I$ & $U^HU = I$ \\ \hline
  \end{tabular}
\end{center}

Recall that a quadratic form over the real numbers
is defined as $q(x) = x^TSx$ for some symmetric matrix $S$,
and diagonalizing $S$ allows us to find a basis
whose vectors all anticommute under the geometric product
and which square to 1, $-1$, or 0.
The theorem which guarantees that this basis exists
is known as the spectral theorem for real matrices,
and an analog of it exists for complex matrices.
However, the complex matrices which may be diagonalized in the right way
are not symmetric but \textit{Hermitian}:
their transpose equals their complex conjugate.
The matrix
$\begin{bmatrix}
  1 & i \\
  i & 1 \\
\end{bmatrix}$
is symmetric but not Hermitian,
since its conjugate is
$\begin{bmatrix}
  1 & -i \\
  -i & 1 \\
\end{bmatrix}$.
$\begin{bmatrix}
  1 & -i \\
  i & 1 \\
\end{bmatrix}$,
on the other hand, is Hermitian.
In other words, a matrix $A$ is Hermitian
if it is equal to its conjugate transpose $A^H$.
A generalization of the spectral theorem states
that all of its eigenvalues are real,
just as for a real symmetric matrix.
However, while negative eigenvalues could only be reduced to $-1$ in the real case
since it could only be divided by the square of a real number (i.e. a positive number),
working in the complex numbers allows us to divide the eigenvalue by $i^2=-1$,
turning any nonzero eigenvalue into 1.
Thus we can find a basis of complex vectors


Notably, using the representation of given above,
any Hermitian complex matrix becomes a symmetric real matrix of twice the size!
However, the reverse is not true:
not all real matrices with even dimensions represent some complex matrix.
This begs the question of the necessary and sufficient criteria
for a real matrix to represent a complex one.
The simplest case is for an arbitrary $2 \times 2$ real matrix
\[
M=
\begin{bmatrix}
  a & c \\
  b & d \\
\end{bmatrix}.
\]
Since the first column uniquely determines the complex number it could represent,
it must be $a+bi$, whose representation is
$\begin{bmatrix}
  a & -b \\
  b & a \\
\end{bmatrix}$.
In order for the original matrix to represent a complex number,
it must be equal to this new matrix,
so the four components of each give us four equations:
\[a = a, b = b, c = -b, d = a.\]
The first two of these are redundant,
but the third and fourth capture the useful information.
Notably, elements are only compared across the diagonal:
$a$ is related to $d$ and $b$ to $c$,
but these pairs can change independently of one another.
Reasoning about particular components of the matrix, though, can only go so far:
as the size of the overall $2n \times 2n$ matrix increases,
the number of equations to keep track of grows proportionally to $n^2$.
Instead, there should be a way to express all of these as a single matrix formula,
and in fact, there is!

Consider the $2 \times 2$ real matrix
\[
\Omega =
\begin{bmatrix}
  0 & -1 \\
  1 & 0
\end{bmatrix}.
\]
It can be seen as the representation of $i$,
or a rotation in 2D space of 90 degrees.
Multiplying another matrix by it on the left
negates the bottom row and switches it with the top:
\[
\Omega M =
\begin{bmatrix}
  0 & -1 \\
  1 & 0
\end{bmatrix}
\begin{bmatrix}
  a & c \\
  b & d
\end{bmatrix} =
\begin{bmatrix}
  -b & -d \\
  a & c
\end{bmatrix}.
\]
Multiplying on the right, on the other hand,
negates the left column and switches it with the right:
\[
M \Omega =
\begin{bmatrix}
  a & c \\
  b & d
\end{bmatrix}
\begin{bmatrix}
  0 & -1 \\
  1 & 0
\end{bmatrix} =
\begin{bmatrix}
  c & -a \\
  d & -b
\end{bmatrix}.
\]
If it is used on both sides,
each of the four entries in the original matrix
ends up in the corner opposite where it started,
since both rows and columns are switched,
potentially with a minus sign:
\[
\Omega M \Omega =
\begin{bmatrix}
  -d & b \\
  c & -a
\end{bmatrix}.
\]
Setting this equal to the original matrix \textit{almost} gives us what we want:
$a$ and $d$ should be equal, not additive inverses,
and vice versa for $b$ and $c$.
This is an easy fix, though:
we may negate one side by changing an $\Omega$ to $-\Omega = \Omega^T$,
the representation of $-i = \bar{i}$:
\[
\begin{bmatrix}
  a & c \\
  b & d
\end{bmatrix} =
M = \Omega M \Omega^T =
\begin{bmatrix}
  d & -b \\
  -c & a
\end{bmatrix}.
\]
This, finally, is the property a $2 \times 2$ real matrix must satisfy
in order to represent a complex number.
For larger matrices, the principle is similar.
$\Omega$ must be extended to a $2n \times 2n$ matrix
with copies of its $2 \times 2$ version on the diagonal,
i.e. the representation of $i$ times the complex identity matrix.
For example, the $6 \times 6$ version would be
\[
\Omega =
\begin{bmatrix}
  0 & -1 & 0 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & -1 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & -1 \\
  0 & 0 & 0 & 0 & 1 & 0
\end{bmatrix},
\]
and a $6 \times 6$ real matrix $M$ represents a $3 \times 3$ complex matrix
if and only if $M = \Omega M \Omega^T$.
(There is no notation necessary to provide the size of a specific $\Omega$ matrix,
just as $I$ refers to an identity matrix with the ``correct'' size.)
Such a matrix $M$ is known as a symplectic matrix.

$\Omega$ has the interesting property that
\[\Omega^{-1} = \Omega^T = -\Omega.\]
$\Omega^{-1} = \Omega^T$ means that $\Omega$ is orthonormal,
$\Omega^T = -\Omega$ means that it is skew-symmetric,
and $\Omega^{-1} = -\Omega$ means that it squares to $-I$. 
Some authors will define $\Omega$ slightly differently,
grouping all of the $1$ and $-1$ entries together to create a block matrix
$
\begin{bmatrix}
  0 & -I \\
  I & 0
\end{bmatrix}
$,
and the definition of a symplectic matrix changes accordingly.
However, the difference amounts to a simple permutation of rows and columns,
and this version of $\Omega$ has the same interesting property described above,
so the underlying structure is practically the same.
However, I believe the relationship between complex matrices and symplectic real matrices
is made clearer with a block diagonal $\Omega$.

\subsection{\texorpdfstring{$\mathbb{C}$}{C} to \texorpdfstring{$\mathbb{H}$}{H}}

Just as a complex number may be represented as a $2 \times 2$ real matrix,
a quaternion may be represented as a $2 \times 2$ complex matrix.

\section{Lie groups and Lie algebras}

\end{document}
