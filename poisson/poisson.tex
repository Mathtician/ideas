\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\graphicspath{images/}

\newcommand{\lparen}{(}
\newcommand{\rparen}{)}
\newcommand{\interval}[2]{\lbrack #1, #2 \rparen}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{MLE for Poisson Process with Increasing Rate}
\author{Aresh Pourkavoos}
\maketitle


Observe a time range $\interval{0}{T}$, obtain events at times $\mathbf{t} = (t_1, \ldots, t_n)$,
where $n \geq 1$, $0 < t_1 < \ldots < t_n < T$ \\
Events are generated by Poisson process with rate function
$\lambda: \interval{0}{T} \rightarrow \interval{0}{\infty}$ \\
Log-likelihood of observation: $\ell(\lambda; \mathbf{t}) = \sum_{i=1}^n \ln(\lambda(t_i)) - \int_0^T\lambda(t)\ \mathrm{d}t$ \\
If $\lambda$ is unrestricted, can obtain arbitrarily high $\ell(\lambda; \mathbf{t})$ by concentrating value around the observations \\
Instead, assume $\lambda$ is (non-strictly) increasing:
how to maximize $\ell(\lambda; \mathbf{t})$ (equivalently, minimize $-\ell(\lambda; \mathbf{t})$)? \\
Given increasing $\lambda$, we may define increasing $\lambda'$ as follows:
\[
\lambda'(t) =
\begin{cases}
  0 & t \in \interval{0}{t_1} \\
  \lambda(t_i) & t \in \interval{t_i}{t_{i+1}} \text{ for } 1 \leq i \leq n \text{ (convention: $t_{n+1} = T$)}
\end{cases}
\]
Intuition: replace the rate before the first event with $0$, extend the rate at each event to the right until the next event \\
Then $\lambda'(t) \leq \lambda(t)$ for all $t$,
so $\int_0^T\lambda'(t)\ \mathrm{d}t \leq \int_0^T\lambda(t)\ \mathrm{d}t$, \\
and $\lambda'(t_i) = \lambda(t_i)$ for all $i$,
so $\sum_{i=1}^n \ln(\lambda'(t_i)) = \sum_{i=1}^n \ln(\lambda(t_i))$ \\
Thus $\ell(\lambda'; \mathbf{t}) \geq \ell(\lambda; \mathbf{t})$,
so the search space may be restricted to all such $\lambda'$: \\
if the MLE among $\lambda'$ does not exist, it does not exist in general,
and if it does, it is the MLE in general \\
From here, $\lambda$ is assumed to be of this form \\
$\lambda$ may be parameterized by $\lambda_1 = \lambda(t_1), \ldots, \lambda_n = \lambda(t_n)$: $\mathbf{\lambda} \in \mathbb{R}^n$ \\

Optimization problem: minimize
\[
-\ell(\mathbf{\lambda}; \mathbf{t}) = \sum_{i=1}^n (t_{i+1}-t_i)\lambda_i-\sum_{i=1}^n \ln(\lambda_i)
\]
subject to
\[
0 < \lambda_1 \leq \ldots \leq \lambda_n
\]
Slight generalization: introduce weights $x_1 < \ldots < x_{n+1}$,
minimize
\[
\sum_{i=1}^n (t_{i+1}-t_i)\lambda_i-\sum_{i=1}^n (x_{i+1}-x_i)\ln(\lambda_i)
\]
with the same constraints on $\lambda$ \\
To recover the original problem, take $x_i = i$ \\
Def: $s_{j,k} = \frac{x_k-x_j}{t_k-t_j}$ for $j < k$ \\
Intuition: represent the problem as points $(t_i, x_i)$; $s_{j,k}$ is the slope of the segment between points $j$ and $k$ \\
Def: $i$ is an \textit{interior} point
if there are $j < i$, $k > i$ s.t.
$s_{j,i} > s_{i,k}$ \\
Otherwise, $i$ is a \textit{hull} point \\
Intuition: hull points are part of the underside of the convex hull of all $(t_i, x_i)$ \\
Ex: $1$ and $n+1$ are vacuously hull points
\begin{lemma}
  If there are no interior points,
  then the optimal values of $\lambda$ are $\lambda_i = s_{i,i+1}$.
\end{lemma}
\begin{proof}
  Function to minimize may be rewritten
  \[
  \sum_{i=1}^n\left((t_{i+1}-t_i)\lambda_i-(x_{i+1}-x_i)\ln(\lambda_i)\right),
  \]
  so term $i$ depends only on $\lambda_i$ \\
  Term $i$ is minimized when $\lambda_i = \frac{x_{i+1}-x_i}{t_{i+1}-t_i} = s_{i,i+1}$ \\
  For all $i \in \{2, \ldots, n\}$,
  take $j = i-1$, $k = i+1$
  in the definition of hull points:
  \[
  \lambda_{i-1} = s_{i-1,i} \leq s_{i,i+1} = \lambda_i,
  \]
  so the constraints are satisfied.
\end{proof}
\begin{lemma}
  If there is an interior point,
  then $s_{i-1,i} > s_{i,i+1}$ for some $i \in \{2, \ldots, n\}$.
\end{lemma}
\begin{proof}
  By contraposition, assume $s_{i-1,i} \leq s_{i,i+1}$ for all $i \in \{2, \ldots, n\}$,
  WTS every point $i$ is a hull point \\
  Let $j < i < k$, WTS $s_{j,i} \leq s_{i,k}$ \\
  Note
  \[
  s_{j,i} = \frac{x_j-x_i}{t_j-t_i}
  = \frac{\sum_{l=j}^{i-1}(x_{l+1}-x_l)}{t_j-t_i}
  = \sum_{l=j}^{i-1}\frac{x_{l+1}-x_l}{t_j-t_i}
  = \sum_{l=j}^{i-1}\left(
  \frac{t_{l+1}-t_l}{t_j-t_i}
  \frac{x_{l+1}-x_l}{t_{l+1}-t_l}
  \right)
  = \sum_{l=j}^{i-1}\left(
  \frac{t_{l+1}-t_l}{t_j-t_i}
  s_{l,l+1}
  \right)
  \]
  Sum of coefficients $\sum_{l=j}^{i-1}\frac{t_{l+1}-t_l}{t_j-t_i} = 1$
  and each one is positive \\
  Thus $s_{j,i}$ is a convex combination of $\{s_{l,l+1} \mid l \in \{j, \ldots, i-1\}\}$ \\
  So $s_{j,i} \leq \max\{s_{l,l+1} \mid l \in \{j, \ldots, i-1\}\} = s_{i-1,i}$ \\
  Similarly, $s_{i,k}$ is a convex combination of $\{s_{l,l+1} \mid l \in \{i, \ldots, k-1\}\}$ \\
  So $s_{i,k} \geq \min\{s_{l,l+1} \mid l \in \{i, \ldots, k-1\}\} = s_{i,i+1}$ \\
  Then $s_{j,i} \leq s_{i-1,i} \leq s_{i,i+1} \leq s_{i,k}$,
  so $i$ is a hull point
\end{proof}
\begin{theorem}
  The optimal solution is $\lambda_i = s_{j,k}$,
  where $j$ is the largest hull point $\leq i$ and $k$ is the smallest hull point $> i$.
\end{theorem}
\begin{proof}
  Induct on the number of interior points: \\
  \begin{itemize}
    \item
      Base case: If there are no interior points, by Lemma 1, the global minimum $\lambda_i = s_{i,i+1}$ is inside the constraint region,
      so it is also the constrained minimum \\
      Also, since all $i$ are hull points, $j = i$ and $k = i+1$ are the correct values given $i$
    \item
      Inductive step: if there is an interior point, by Lemma 2, the global minimum $(s_{1,2}, \ldots, s_{n,n+1})$ is outside the constraint region \\
      Draw a line segment between the global min and the constrained min \\
      Where the segment intersects the boundary of the constraint region, $\lambda_{i-1} = \lambda_i$ for some $i$ such that $s_{i-1,i} > s_{i,i+1}$ (so $i$ is an interior point) \\
      This intersection is also a constrained minimum because the function being optimized is convex \\
      Thus we may restrict the search space to $\lambda_{i-1} = \lambda_i$,
      which is equivalent to removing the point $(t_i, x_i)$ from the problem:
      the terms $(t_i-t_{i-1})\lambda_i+(t_{i+1}-t_i)\lambda_{i+1}$ become $(t_{i+1}-t_{i-1})\lambda_i$, and similarly for the $x$ terms \\
      Removing this point does not change the set of hull points \\
      By the IH, the constrained minimum of the new problem in $1$ fewer variable is given by the hull points as described in the theorem statement \\
      Thus, so is the minimum in the original problem
      %% Since this is a convex optimization problem and the constraint region is closed,
      %% the solution is on the boundary \\
      %% Furthermore, in the optimal solution, $\lambda_i=\lambda_{i+1}$ for some $i$ where $s_{i-1,i} > s_{i,i+1}$ 
      %% (consider the line segment between $(s_{1,2}, \ldots, s_{n,n+1})$ and the optimal solution) \\
      %% Thus we can rewrite the problem by equating $\lambda_i$ and $\lambda_{i+1}$,
      %% effectively removing the point $(t_i, x_i)$:
  \end{itemize}
\end{proof}
%% \begin{lemma}
%%   If there is an interior point,
%%   then $\lambda_{i-1} = \lambda_i$ for some interior point $i$.
%%   (This is not saying that $\lambda_{i-1} = \lambda_i$ for all interior points $i$,
%%   which wil be shown later.)
%% \end{lemma}
%% \begin{proof}
%%   Suppose there is an interior point \\
%%   Consider the point $S = (s_{1,2}, \ldots, s_{n,n+1})$ in the positive orthant of $\mathbb{R}^n$ \\
%%   By the previous lemma, $s_{i-1, i} > s_{i,i+1}$ for some $i$
%%   (which is also an interior point: take $j = i-1$, $k = i+1$) \\
%%   Thus $S$ lies outside the constraint region $\lambda_1 \leq \ldots \leq \lambda_n$ \\
%%   Draw segment between $S$ and the optimal solution $\lambda^*$ in the region \\
%%   Since the region is closed and convex, the segment intersects the boundary of the region at some $T$ \\
%%   Since the function to minimize is convex, its value at $T$ is $\geq$ the min. of the values at $S$ and $\lambda^*$ \\
%%   $S$ is the global optimum, so the value at $T$ is $\geq$ that of $\lambda^*$ \\
%%   Thus $T$
  
%% \end{proof}
%% Lemma: Let $f$ be a convex function over convex $C \subseteq \mathbb{R}^n$,
%% let $D \subseteq \mathbb{R}^n$ be closed and convex, and suppose $C \cap D$ is nonempty.
%% If $f$ attains min over $C$ at $\mathbf{x} \not\in D$, then
%% $f$ attains min over $C \cap D$ at some $\mathbf{w} \in \partial D$.

%% Proof: Let $\mathbf{y}$ be a point in $C \cap D$ where $f$ is minimized.
%% Then the segment between $\mathbf{x}$ and $\mathbf{y}$ intersects $\partial D$
%% (if $\mathbf{y} \in \partial D$, at $\mathbf{y}$ itself).
%% Call this intersection $\mathbf{z}$.
%% Then $f(\mathbf{z}) \leq f(\mathbf{y})$ by convexity. \\

%% Apply lemma with $C = (\mathbb{R}^+)^n$, $D = \{\boldsymbol{\lambda} = (\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n\ |\ \lambda_1 \leq \ldots \leq \lambda_n\}$

%% WTS: if unconstrained problem has $\lambda_i > \lambda_{i+1}$ for some $i$,
%% then constrained problem has $\lambda_i = \lambda_{i+1}$
%% (for one of those $i$, not an arbitrary one at first) \\
%% Next, WTS 

%% Constraint region may be partitioned into $2^{n-1}$ sets,
%% one for each $I \subseteq \{1, \ldots, n-1\}$: \\
%% $R_I = \{(\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n\ |\ 0 < \lambda_1,\
%% \forall i \in I (\lambda_i < \lambda_{i+1}),\ \forall i \not\in I (\lambda_i = \lambda_{i+1}) \}$ \\
%% Each $R_I$ may be parameterized by $\lambda_1$ and $\lambda_{i+1}$ for each $i \in I$ \\
%% Ex: if $n = 4$, $R_{\{2\}} = \{(\lambda_1, \lambda_1, \lambda_3, \lambda_3)\ |\ 0 < \lambda_1 < \lambda_3\}$ \\
%% Param space is open, so if $\ell$ attains max in $R_I$, then gradient at max wrt params is 0 \\
%% May take unions of these regions to \\
%% $S_J = \bigcup_{I \subseteq J} R_I = \{(\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n\ |\ 0 < \lambda_1,\
%% \forall i \in I (\lambda_i \leq \lambda_{i+1}),\ \forall i \not\in I (\lambda_i < \lambda_{i+1}) \}$

%% \newpage
%% %% Finite set $S$, function $g: \mathcal{P}(S) \rightarrow \mathcal{P}(S)$ \\
%% %% $X \subseteq g(X)$, $X \subseteq Y \implies g(X) \subseteq g(Y)$ \\
%% Integral of MLE is underside of convex hull of $\{(t_i, i)\ |\ 0 \leq i \leq n+1\}$ ($t_0 = 0$) \\
%% May help find bias/variance

\end{document}
