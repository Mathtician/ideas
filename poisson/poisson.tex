\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{images/}

\newcommand{\lparen}{(}
\newcommand{\rparen}{)}
\newcommand{\interval}[2]{\lbrack #1, #2 \rparen}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{MLE for Poisson Process with Increasing Rate}
\author{Aresh Pourkavoos}
\maketitle


Observe time range $\interval{0}{T}$, obtain events at times $\mathbf{t} = (t_1, \ldots, t_n)$,
where $n \geq 1$, $0 < t_1 < \ldots < t_n < T$ \\
Events generated by Poisson process with rate function
$\lambda: \interval{0}{T} \rightarrow \interval{0}{\infty}$ \\
Log-likelihood of observation: $\ell(\lambda; \mathbf{t}) = \sum_{i=1}^n \ln(\lambda(t_i)) - \int_0^T\lambda(t)\ \mathrm{d}t$ \\
If $\lambda$ unrestricted, can obtain arbitrarily high $\ell(\lambda; \mathbf{t})$ by concentrating value around observations \\
Instead, assume $\lambda$ (non-strictly) increasing:
how to maximize $\ell(\lambda; \mathbf{t})$ (equivalently, minimize $-\ell(\lambda; \mathbf{t})$)? \\
Given increasing $\lambda$, may define increasing $\lambda'$ as follows:
\[
\lambda'(t) =
\begin{cases}
  0 & t \in \interval{0}{t_1} \\
  \lambda(t_i) & t \in \interval{t_i}{t_{i+1}} \text{ for } 1 \leq i \leq n \text{ (convention: $t_{n+1} = T$)}
\end{cases}
\]
Intuition: replace rate before first event with $0$, extend rate at each event to the right until next event \\
Then $\lambda'(t) \leq \lambda(t)$ for all $t$,
so $\int_0^T\lambda'(t)\ \mathrm{d}t \leq \int_0^T\lambda(t)\ \mathrm{d}t$, \\
and $\lambda'(t_i) = \lambda(t_i)$ for all $i$,
so $\sum_{i=1}^n \ln(\lambda'(t_i)) = \sum_{i=1}^n \ln(\lambda(t_i))$ \\
Thus $\ell(\lambda'; \mathbf{t}) \geq \ell(\lambda; \mathbf{t})$,
so search space may be restricted to all such $\lambda'$: \\
if MLE among $\lambda'$ does not exist, it does not exist in general,
and if it does, it is MLE in general \\
From here, $\lambda$ assumed to be of this form \\
$\lambda$ may be parameterized by $\lambda_1 = \lambda(t_1), \ldots, \lambda_n = \lambda(t_n)$ \\
Then $\ell(\lambda; \mathbf{t}) = \sum_{i=1}^n \ln(\lambda_i) - \sum_{i=1}^n s_i\lambda_i$,
where $s_i = t_{i+1}-t_i > 0$ \\
$\lambda$ increasing, so $0 < \lambda_1 \leq \ldots \leq \lambda_n$ \\

Lemma: Let $f$ be a convex function over convex $C \subseteq \mathbb{R}^n$,
let $D \subseteq \mathbb{R}^n$ be closed and convex, and suppose $C \cap D$ is nonempty.
If $f$ attains min over $C$ at $\mathbf{x} \not\in D$, then
$f$ attains min over $C \cap D$ at some $\mathbf{w} \in \partial D$.

Proof: Let $\mathbf{y}$ be a point in $C \cap D$ where $f$ is minimized.
Then the segment between $\mathbf{x}$ and $\mathbf{y}$ intersects $\partial D$
(if $\mathbf{y} \in \partial D$, at $\mathbf{y}$ itself).
Call this intersection $\mathbf{z}$.
Then $f(\mathbf{z}) \leq f(\mathbf{y})$ by convexity. \\

Apply lemma with $C = (\mathbb{R}^+)^n$, $D = \{\boldsymbol{\lambda} = (\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n\ |\ \lambda_1 \leq \ldots \leq \lambda_n\}$

WTS: if unconstrained problem has $\lambda_i > \lambda_{i+1}$ for some $i$,
then constrained problem has $\lambda_i = \lambda_{i+1}$
(for one of those $i$, not an arbitrary one at first) \\
Next, WTS 

%% Constraint region may be partitioned into $2^{n-1}$ sets,
%% one for each $I \subseteq \{1, \ldots, n-1\}$: \\
%% $R_I = \{(\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n\ |\ 0 < \lambda_1,\
%% \forall i \in I (\lambda_i < \lambda_{i+1}),\ \forall i \not\in I (\lambda_i = \lambda_{i+1}) \}$ \\
%% Each $R_I$ may be parameterized by $\lambda_1$ and $\lambda_{i+1}$ for each $i \in I$ \\
%% Ex: if $n = 4$, $R_{\{2\}} = \{(\lambda_1, \lambda_1, \lambda_3, \lambda_3)\ |\ 0 < \lambda_1 < \lambda_3\}$ \\
%% Param space is open, so if $\ell$ attains max in $R_I$, then gradient at max wrt params is 0 \\
%% May take unions of these regions to \\
%% $S_J = \bigcup_{I \subseteq J} R_I = \{(\lambda_1, \ldots, \lambda_n) \in \mathbb{R}^n\ |\ 0 < \lambda_1,\
%% \forall i \in I (\lambda_i \leq \lambda_{i+1}),\ \forall i \not\in I (\lambda_i < \lambda_{i+1}) \}$

\newpage
%% Finite set $S$, function $g: \mathcal{P}(S) \rightarrow \mathcal{P}(S)$ \\
%% $X \subseteq g(X)$, $X \subseteq Y \implies g(X) \subseteq g(Y)$ \\
Integral of MLE is underside of convex hull of $\{(t_i, i)\ |\ 0 \leq i \leq n+1\}$ ($t_0 = 0$) \\
May help find bias/variance

\end{document}
